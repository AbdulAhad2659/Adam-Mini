{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install adam-mini\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import warnings\n",
        "\n",
        "try:\n",
        "    import triton\n",
        "    import triton.language as tl\n",
        "    HAS_TRITON = True\n",
        "except ImportError:\n",
        "    HAS_TRITON = False\n",
        "    print(\"Triton not installed. Triton optimizer unavailable.\")\n",
        "\n",
        "# Map string to Triton math and PyTorch dtypes\n",
        "_DTYPE_MAP = {}\n",
        "if HAS_TRITON:\n",
        "    _DTYPE_MAP = {\n",
        "        \"bf16\": (tl.bfloat16, torch.bfloat16),\n",
        "        \"fp16\": (tl.float16, torch.float16),\n",
        "        \"fp32\": (tl.float32, torch.float32),\n",
        "        \"fp8e4\": (tl.float8e4nv, getattr(torch, \"float8_e4m3fn\", None))\n",
        "    }\n",
        "\n",
        "_warn_once_cache = set()\n",
        "def warn_once(msg: str):\n",
        "    if msg not in _warn_once_cache:\n",
        "        warnings.warn(msg)\n",
        "        _warn_once_cache.add(msg)\n",
        "\n",
        "# Triton kernels\n",
        "if HAS_TRITON:\n",
        "    @triton.jit\n",
        "    def _reduce_g2_kernel_parallel(g_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
        "        # This kernel sums g^2 over a tile of size BLOCK_SIZE and atomically adds to output_ptr.\n",
        "        pid = tl.program_id(axis=0)\n",
        "        offset = pid * BLOCK_SIZE\n",
        "        idx = offset + tl.arange(0, BLOCK_SIZE)\n",
        "        mask = idx < n_elements\n",
        "        g = tl.load(g_ptr + idx, mask=mask, other=0.0).to(tl.float32)\n",
        "        sum_g2 = tl.sum(g * g, axis=0)\n",
        "        tl.atomic_add(output_ptr, sum_g2)\n",
        "\n",
        "    @triton.jit\n",
        "    def _update_kernel(\n",
        "        p_ptr, g_ptr, m_ptr, amax_m_ptr,\n",
        "        lr_p, beta1_p, weight_decay_p, bias_correction1_p,\n",
        "        step_scale_ptr, n_elements,\n",
        "        BLOCK_SIZE: tl.constexpr,\n",
        "        T_MATH: tl.constexpr, T_STATE: tl.constexpr, STATE_IS_FP8: tl.constexpr\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Triton kernel to fuse one optimization step:\n",
        "          - Loads parameter p, gradient g, momentum m (optionally in FP8), each as vectors.\n",
        "          - Applies AdamW-style weight decay: p = p * (1 - lr*wd).\n",
        "          - Updates momentum m_new = beta1*m + (1-beta1)*g.\n",
        "          - Computes bias-corrected m_hat = m_new * bias_correction1.\n",
        "          - Updates parameter p_new = p - step_scale * m_hat.\n",
        "          - Writes back p_new (original dtype) and m_new (to state dtype).\n",
        "          - Updates amax_m_ptr with max(|m_new|) for FP8 saturation check.\n",
        "        \"\"\"\n",
        "        pid = tl.program_id(axis=0)\n",
        "        offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "        mask = offsets < n_elements\n",
        "\n",
        "        # Load parameter and gradient\n",
        "        p = tl.load(p_ptr + offsets, mask=mask).to(T_MATH)\n",
        "        g = tl.load(g_ptr + offsets, mask=mask).to(T_MATH)\n",
        "        # Load previous momentum (FP8 or not)\n",
        "        if STATE_IS_FP8:\n",
        "            m = tl.load(m_ptr + offsets, mask=mask).to(T_STATE).to(T_MATH)\n",
        "        else:\n",
        "            m = tl.load(m_ptr + offsets, mask=mask).to(T_MATH)\n",
        "\n",
        "        # Broadcast scalars\n",
        "        lr = tl.full((), lr_p, T_MATH)\n",
        "        beta1 = tl.full((), beta1_p, T_MATH)\n",
        "        wd = tl.full((), weight_decay_p, T_MATH)\n",
        "        bc1 = tl.full((), bias_correction1_p, T_MATH)\n",
        "        step_scale = tl.load(step_scale_ptr).to(T_MATH)\n",
        "\n",
        "        # Decoupled weight decay (p = p * (1 - lr*wd))\n",
        "        if wd != 0:\n",
        "            p = p * (1 - lr * wd)\n",
        "\n",
        "        # Momentum update (no extra grad scaling)\n",
        "        m_new = beta1 * m + (1 - beta1) * g\n",
        "        m_hat = m_new * bc1\n",
        "        p_new = p - step_scale * m_hat\n",
        "\n",
        "        # Track max(|m_new|) for FP8 overflow check\n",
        "        max_m_val = tl.max(tl.abs(m_new.to(tl.float32)))\n",
        "        tl.atomic_max(amax_m_ptr, max_m_val)\n",
        "\n",
        "        # Write updated parameter and momentum\n",
        "        tl.store(p_ptr + offsets, p_new.to(p_ptr.dtype.element_ty), mask=mask)\n",
        "        tl.store(m_ptr + offsets, m_new.to(T_STATE), mask=mask)\n",
        "\n",
        "# PyTorch (reference) implementation of Adam-mini\n",
        "class AdamMiniPyTorch(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "        self.block_states = {}\n",
        "\n",
        "    def _init_groups_and_states(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                # First moment (exp_avg) per parameter\n",
        "                self.state[p][\"exp_avg\"] = torch.zeros_like(p)\n",
        "                # v_bar per \"block\" (here each parameter is its own block)\n",
        "                self.block_states[id(p)] = {\n",
        "                    \"v_bar\": torch.zeros(1, device=p.device, dtype=torch.float32),\n",
        "                    \"step\": 0\n",
        "                }\n",
        "        self.state[\"initialized\"] = True\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        if not self.state.get(\"initialized\", False):\n",
        "            self._init_groups_and_states()\n",
        "        for group in self.param_groups:\n",
        "            beta1, beta2 = group[\"betas\"]\n",
        "            lr, eps, wd = group[\"lr\"], group[\"eps\"], group[\"weight_decay\"]\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "                block = self.block_states[id(p)]\n",
        "\n",
        "                # Increment step for this block\n",
        "                block[\"step\"] += 1\n",
        "                step = block[\"step\"]\n",
        "\n",
        "                # Block-wise second moment: moving average of mean(g^2)\n",
        "                g2 = (grad.to(torch.float32) ** 2).mean()\n",
        "                v_bar = block[\"v_bar\"]\n",
        "                v_bar.mul_(beta2).add_(g2, alpha=1 - beta2)\n",
        "\n",
        "                # Bias corrections\n",
        "                bc1 = 1.0 - beta1 ** step\n",
        "                bc2 = 1.0 - beta2 ** step\n",
        "\n",
        "                # Compute step size = lr / (sqrt(v_bar/bc2) + eps)\n",
        "                denom = (v_bar / bc2).sqrt() + eps\n",
        "                step_scale = lr / denom\n",
        "\n",
        "                # Decoupled weight decay (AdamW)\n",
        "                if wd != 0:\n",
        "                    p.mul_(1 - lr * wd)\n",
        "\n",
        "                # Update first moment\n",
        "                exp_avg = state[\"exp_avg\"]\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                m_hat = exp_avg / bc1\n",
        "                # Parameter update\n",
        "                p.add_(-step_scale * m_hat)\n",
        "\n",
        "                # Telemetry for debugging\n",
        "                state.setdefault(\"telemetry\", {\n",
        "                    \"steps\": 0,\n",
        "                    \"max_update\": [],\n",
        "                    \"max_m\": [],\n",
        "                    \"dtype\": str(exp_avg.dtype)\n",
        "                })\n",
        "                state[\"telemetry\"][\"steps\"] += 1\n",
        "                # Record max parameter update magnitude (|step_scale * m_hat|)\n",
        "                max_update_val = (step_scale * m_hat).abs().max().item()\n",
        "                state[\"telemetry\"][\"max_update\"].append(max_update_val)\n",
        "                # Record max momentum magnitude\n",
        "                state[\"telemetry\"][\"max_m\"].append(exp_avg.abs().max().item())\n",
        "\n",
        "# Triton-optimized Adam-mini\n",
        "class AdamMiniTriton(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0,\n",
        "                 math_dtype=\"bf16\", state_dtype=\"bf16\"):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "        self.math_dtype_str = math_dtype\n",
        "        self.state_dtype_str = state_dtype\n",
        "        self.block_states = {}\n",
        "\n",
        "    def _init_groups_and_states(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                # Determine storage dtype for momentum\n",
        "                resolved_state = self.state_dtype_str\n",
        "                if resolved_state == \"fp8e4\" and _DTYPE_MAP[\"fp8e4\"][1] is None:\n",
        "                    resolved_state = \"bf16\"\n",
        "                self.state[p][\"exp_avg\"] = torch.zeros_like(p, dtype=_DTYPE_MAP[resolved_state][1])\n",
        "                self.state[p][\"amax_m\"] = torch.zeros(1, device=p.device, dtype=torch.float32)\n",
        "                self.block_states[id(p)] = {\n",
        "                    \"v_bar\": torch.zeros(1, device=p.device, dtype=torch.float32),\n",
        "                    \"step\": 0,\n",
        "                    \"state_is_fp8\": (resolved_state == \"fp8e4\")\n",
        "                }\n",
        "        self.state[\"initialized\"] = True\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        if not self.state.get(\"initialized\", False):\n",
        "            self._init_groups_and_states()\n",
        "        for group in self.param_groups:\n",
        "            beta1, beta2 = group[\"betas\"]\n",
        "            lr, eps, wd = group[\"lr\"], group[\"eps\"], group[\"weight_decay\"]\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "                block = self.block_states[id(p)]\n",
        "\n",
        "                # Update step and v_bar\n",
        "                block[\"step\"] += 1\n",
        "                step = block[\"step\"]\n",
        "                g2 = (grad.to(torch.float32) ** 2).mean()\n",
        "                v_bar = block[\"v_bar\"]\n",
        "                v_bar.mul_(beta2).add_(g2, alpha=1 - beta2)\n",
        "\n",
        "                # Bias corrections\n",
        "                bc1 = 1.0 - beta1 ** step\n",
        "                bc2 = 1.0 - beta2 ** step\n",
        "\n",
        "                # Step scale for this block\n",
        "                denom = (v_bar / bc2).sqrt() + eps\n",
        "                step_scale = (lr / denom).to(torch.bfloat16)\n",
        "                step_scale_val = float(step_scale)\n",
        "\n",
        "                m_buffer = state[\"exp_avg\"]\n",
        "                amax_m = state[\"amax_m\"].zero_()\n",
        "\n",
        "                # Launch Triton kernel (one tile per block of size 1024)\n",
        "                _update_kernel[(triton.cdiv(p.numel(), 1024),)](\n",
        "                    p, grad, m_buffer, amax_m,\n",
        "                    lr, beta1, wd, 1.0 / bc1,\n",
        "                    step_scale, p.numel(),\n",
        "                    BLOCK_SIZE=1024,\n",
        "                    T_MATH=_DTYPE_MAP[self.math_dtype_str][0],\n",
        "                    T_STATE=_DTYPE_MAP[self.state_dtype_str][0],\n",
        "                    STATE_IS_FP8=block[\"state_is_fp8\"]\n",
        "                )\n",
        "\n",
        "                # FP8 overflow check: switch to BF16 if needed\n",
        "                if amax_m.item() > 400.0 and block[\"state_is_fp8\"]:\n",
        "                    warn_once(\"FP8 momentum saturated; switching block to BF16.\")\n",
        "                    block[\"state_is_fp8\"] = False\n",
        "                    state[\"exp_avg\"] = state[\"exp_avg\"].to(torch.bfloat16)\n",
        "\n",
        "                # Telemetry\n",
        "                state.setdefault(\"telemetry\", {\n",
        "                    \"steps\": 0,\n",
        "                    \"max_update\": [],\n",
        "                    \"max_m\": [],\n",
        "                    \"dtype\": str(m_buffer.dtype)\n",
        "                })\n",
        "                state[\"telemetry\"][\"steps\"] += 1\n",
        "                # Compute actual max parameter update magnitude, analogous to PyTorch version\n",
        "                if bc1 != 0:\n",
        "                    bc1_inv = 1.0 / bc1\n",
        "                else:\n",
        "                    bc1_inv = 0.0\n",
        "                m_fp32 = m_buffer.to(torch.float32)\n",
        "                max_update_val = (step_scale_val * m_fp32 * bc1_inv).abs().max().item()\n",
        "                max_m_val = m_fp32.abs().max().item()\n",
        "                state[\"telemetry\"][\"max_update\"].append(max_update_val)\n",
        "                state[\"telemetry\"][\"max_m\"].append(max_m_val)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Toy model & parity test\n",
        "# ---------------------------------------------------------------------\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(16, 16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "def parity_check(optA_cls, optB_cls, steps=5):\n",
        "    torch.manual_seed(0)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    modelA = SimpleModel().to(device)\n",
        "    modelB = SimpleModel().to(device)\n",
        "    modelB.load_state_dict(modelA.state_dict())  # sync weights\n",
        "\n",
        "    optA = optA_cls(modelA.parameters(), lr=1e-3)\n",
        "    optB = optB_cls(modelB.parameters(), lr=1e-3)\n",
        "\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    for step in range(steps):\n",
        "        data = torch.randn(8, 16, device=device)\n",
        "        target = torch.randn(8, 16, device=device)\n",
        "\n",
        "        # Run model A\n",
        "        outA = modelA(data)\n",
        "        lossA = loss_fn(outA, target)\n",
        "        optA.zero_grad()\n",
        "        lossA.backward()\n",
        "        optA.step()\n",
        "\n",
        "        # Run model B\n",
        "        outB = modelB(data)\n",
        "        lossB = loss_fn(outB, target)\n",
        "        optB.zero_grad()\n",
        "        lossB.backward()\n",
        "        optB.step()\n",
        "\n",
        "    # Compare weights and telemetry\n",
        "    for (nA, pA), (nB, pB) in zip(modelA.named_parameters(), modelB.named_parameters()):\n",
        "        diff = (pA - pB).abs().max().item()\n",
        "        print(f\"Param {nA}: max diff {diff:.6e}\", \"\\n\")\n",
        "        # Retrieve telemetry from each optimizer\n",
        "        telA = optA.state[pA][\"telemetry\"]\n",
        "        telB = optB.state[pB][\"telemetry\"]\n",
        "        print(f\"  [Telemetry] Steps={telA['steps']}, max_update_A={telA['max_update']}, max_m_A={telA['max_m']}, dtype_A={telA['dtype']}\")\n",
        "        print(f\"              Steps={telB['steps']}, max_update_B={telB['max_update']}, max_m_B={telB['max_m']}, dtype_B={telB['dtype']}\")\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# MAIN\n",
        "# ---------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Running parity test between AdamMiniPyTorch and AdamMiniTriton...\\n\")\n",
        "    parity_check(AdamMiniPyTorch, AdamMiniTriton)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es1tGexez4H8",
        "outputId": "fb450c88-c133-4c0b-bf55-f66312d2a284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running parity test between AdamMiniPyTorch and AdamMiniTriton...\n",
            "\n",
            "Param linear.weight: max diff 2.246886e-03 \n",
            "\n",
            "  [Telemetry] Steps=5, max_update_A=[0.004441387485712767, 0.0023785014636814594, 0.00216480134986341, 0.0015761416871100664, 0.0015192623250186443], max_m_A=[0.02251160517334938, 0.026076965034008026, 0.03351600840687752, 0.029425164684653282, 0.03513770550489426], dtype_A=torch.float32\n",
            "              Steps=5, max_update_B=[0.004538297653198242, 0.002416654722765088, 0.002185906982049346, 0.001596447778865695, 0.0015406586462631822], max_m_B=[0.02294921875, 0.0264892578125, 0.033935546875, 0.02978515625, 0.03564453125], dtype_B=torch.bfloat16\n",
            "\n",
            "\n",
            "\n",
            "Param linear.bias: max diff 1.433402e-03 \n",
            "\n",
            "  [Telemetry] Steps=5, max_update_A=[0.002155275084078312, 0.001265847124159336, 0.001437773578800261, 0.0010939586209133267, 0.0008378218044526875], max_m_A=[0.009643001481890678, 0.015522426925599575, 0.023487450554966927, 0.022799750789999962, 0.021312423050403595], dtype_A=torch.float32\n",
            "              Steps=5, max_update_B=[0.002181529998779297, 0.001284872181713581, 0.0014582243748009205, 0.0011055630166083574, 0.0008453609189018607], max_m_B=[0.009765625, 0.0157470703125, 0.0238037109375, 0.0230712890625, 0.021484375], dtype_B=torch.bfloat16\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adam-mini\n",
        "\n",
        "# === START OF FILE ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from typing import Callable, List, Tuple, Dict, Optional, Iterable\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# ======================================================================\n",
        "# Setup: Triton import (optional) and dtype map\n",
        "# ======================================================================\n",
        "try:\n",
        "    import triton\n",
        "    import triton.language as tl\n",
        "    HAS_TRITON = True\n",
        "except ImportError:\n",
        "    HAS_TRITON = False\n",
        "    print('Triton is not installed. Fused kernel will not be available.')\n",
        "\n",
        "_DTYPE_MAP = {}\n",
        "if HAS_TRITON:\n",
        "    _DTYPE_MAP = {\n",
        "        \"bf16\": (tl.bfloat16, torch.bfloat16), \"fp16\": (tl.float16, torch.float16),\n",
        "        \"fp32\": (tl.float32, torch.float32), \"fp8e4\": (tl.float8e4nv, getattr(torch, 'float8e4m3fn', None))\n",
        "    }\n",
        "\n",
        "_warn_once_cache = set()\n",
        "def warn_once(msg: str):\n",
        "    if msg not in _warn_once_cache:\n",
        "        warnings.warn(msg); _warn_once_cache.add(msg)\n",
        "\n",
        "# ======================================================================\n",
        "# Feature: Hessian-Aware Partitioner for Transformers\n",
        "# ======================================================================\n",
        "def adam_mini_transformer_partition(model: nn.Module) -> List[Tuple[str, List[Tuple[str, torch.Tensor]], Dict]]:\n",
        "    # --- *** THE FIX IS HERE: Initialize the 'partitions' dictionary *** ---\n",
        "    partitions = defaultdict(list)\n",
        "    param_to_group_map = {}\n",
        "\n",
        "    for group in model.param_groups:\n",
        "        for p in group['params']:\n",
        "            if p.requires_grad: param_to_group_map[p] = group\n",
        "\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad: continue\n",
        "        low_name = name.lower()\n",
        "        if 'bias' in low_name or 'norm' in low_name or 'ln' in low_name: partitions['bias_and_norm'].append((name, p))\n",
        "        elif any(key in low_name for key in {\"embed\", \"wte\", \"wpe\", \"lm_head\", \"output\"}): partitions[f'embd_output_{name}'] = [(name, p)]\n",
        "        elif any(key in low_name for key in {\"k_proj\", \"q_proj\", \"wk\", \"wq\"}): partitions[f'q_and_k_{name}'] = [(name, p)]\n",
        "        elif any(key in low_name for key in {\"v_proj\", \"wv\", \"o_proj\", \"wo\"}): partitions[f'v_and_proj_{name}'] = [(name, p)]\n",
        "        elif 'mlp' in low_name: partitions[f'mlp_{name}'] = [(name, p)]\n",
        "        else: partitions[f'other_{name}'] = [(name, p)]\n",
        "\n",
        "    final_blocks = []\n",
        "    for block_key, params_list in partitions.items():\n",
        "        if not params_list: continue\n",
        "        first_param_group = param_to_group_map[params_list[0][1]]\n",
        "        final_blocks.append((block_key, params_list, first_param_group))\n",
        "\n",
        "    print(f\"Hessian-aware partitioner created {len(final_blocks)} blocks.\")\n",
        "    return final_blocks\n",
        "\n",
        "# ======================================================================\n",
        "# Triton Kernels (Unchanged)\n",
        "# ======================================================================\n",
        "if HAS_TRITON:\n",
        "    @triton.jit\n",
        "    def _reduce_g2_kernel_parallel(g_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
        "        pid = tl.program_id(axis=0); offset = pid * BLOCK_SIZE\n",
        "        idx = offset + tl.arange(0, BLOCK_SIZE); mask = idx < n_elements\n",
        "        g = tl.load(g_ptr + idx, mask=mask, other=0.0).to(tl.float32)\n",
        "        sum_g2 = tl.sum(g * g, axis=0)\n",
        "        tl.atomic_add(output_ptr, sum_g2)\n",
        "\n",
        "    @triton.autotune(\n",
        "        configs=[triton.Config({'BLOCK_SIZE': 1024}, num_warps=8), triton.Config({'BLOCK_SIZE': 2048}, num_warps=8)],\n",
        "        key=['n_elements'],\n",
        "    )\n",
        "    @triton.jit\n",
        "    def _update_kernel(\n",
        "        p_ptr, g_ptr, m_ptr, amax_m_ptr, lr_p, beta1_p, weight_decay_p, bias_correction1_p,\n",
        "        step_scale_ptr, state_scale_p, n_elements,\n",
        "        BLOCK_SIZE: tl.constexpr, T_MATH: tl.constexpr, T_STATE: tl.constexpr, STATE_IS_FP8: tl.constexpr,\n",
        "    ):\n",
        "        pid = tl.program_id(axis=0); offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE); mask = offsets < n_elements\n",
        "        p = tl.load(p_ptr + offsets, mask=mask).to(T_MATH); g = tl.load(g_ptr + offsets, mask=mask).to(T_MATH)\n",
        "        if STATE_IS_FP8: m_scaled = tl.load(m_ptr + offsets, mask=mask).to(T_STATE).to(T_MATH)\n",
        "        else: m_scaled = tl.load(m_ptr + offsets, mask=mask).to(T_MATH)\n",
        "        lr = tl.full((), lr_p, T_MATH); beta1 = tl.full((), beta1_p, T_MATH); weight_decay = tl.full((), weight_decay_p, T_MATH)\n",
        "        bias_correction1 = tl.full((), bias_correction1_p, T_MATH); state_scale = tl.full((), state_scale_p, T_MATH)\n",
        "        step_scale = tl.load(step_scale_ptr).to(T_MATH)\n",
        "        if weight_decay != 0: p = p * (1 - lr * weight_decay)\n",
        "        m_unscaled = m_scaled / state_scale; m_new_unscaled = beta1 * m_unscaled + (1 - beta1) * g\n",
        "        m_hat = m_new_unscaled * bias_correction1; p_new = p - step_scale * m_hat\n",
        "        max_m_val = tl.max(tl.abs(m_new_unscaled.to(tl.float32))); tl.atomic_max(amax_m_ptr, max_m_val)\n",
        "        tl.store(p_ptr + offsets, p_new.to(p_ptr.dtype.element_ty), mask=mask)\n",
        "        if STATE_IS_FP8: tl.store(m_ptr + offsets, (m_new_unscaled * state_scale).to(T_STATE), mask=mask)\n",
        "        else: tl.store(m_ptr + offsets, m_new_unscaled.to(m_ptr.dtype.element_ty), mask=mask)\n",
        "\n",
        "# ======================================================================\n",
        "# Final Optimizer Class\n",
        "# ======================================================================\n",
        "class AdamMiniTriton(Optimizer):\n",
        "    def __init__(self, params, lr: float = 1e-3, betas: tuple = (0.9, 0.999), eps: float = 1e-6,\n",
        "                 weight_decay: float = 0.0, math_dtype: str = \"bf16\", state_dtype: str = \"bf16\",\n",
        "                 partition_fn: Optional[Callable] = None, model_for_partitioning: Optional[nn.Module] = None):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "        self.math_dtype_str = math_dtype; self.state_dtype_str = state_dtype\n",
        "        self.partition_fn = partition_fn; self.model_for_partitioning = model_for_partitioning\n",
        "        self.block_states = {}; self.telemetry = defaultdict(list); self._blocks_built = False\n",
        "\n",
        "    def get_telemetry(self): return dict(self.telemetry)\n",
        "\n",
        "    def _resolve_state_dtype(self, requested_dtype, device):\n",
        "        if \"fp8\" in requested_dtype:\n",
        "            if _DTYPE_MAP[requested_dtype][1] is None: warn_once(f\"PyTorch version lacks {requested_dtype}. Falling back to bf16.\"); return \"bf16\"\n",
        "            major, _ = torch.cuda.get_device_capability(device)\n",
        "            if major < 9: warn_once(\"FP8 requires SM90+; falling back to bf16 state.\"); return \"bf16\"\n",
        "        return requested_dtype\n",
        "\n",
        "    def _init_groups_and_states(self):\n",
        "        if self.partition_fn:\n",
        "            if self.model_for_partitioning is None: raise ValueError(\"model_for_partitioning must be provided for partition_fn.\")\n",
        "            # Ensure the model has access to the optimizer's param_groups\n",
        "            self.model_for_partitioning.param_groups = self.param_groups\n",
        "            self.param_blocks = self.partition_fn(self.model_for_partitioning)\n",
        "        else:\n",
        "            self.param_blocks = []\n",
        "            for group in self.param_groups:\n",
        "                params_with_grad = [p for p in group['params'] if p.grad is not None]\n",
        "                if params_with_grad: self.param_blocks.append((f\"group_{id(group)}\", [(p.name if hasattr(p, 'name') else f'param_{i}', p) for i, p in enumerate(params_with_grad)], group))\n",
        "\n",
        "        for block_name, block_params, group in self.param_blocks:\n",
        "            first_param = block_params[0][1]; device = first_param.device\n",
        "            resolved_state_dtype_str = self._resolve_state_dtype(self.state_dtype_str, device)\n",
        "            _, torch_state = _DTYPE_MAP[resolved_state_dtype_str]; is_fp8 = \"fp8\" in resolved_state_dtype_str\n",
        "            self.block_states[block_name] = {'v_bar_fp32': torch.zeros(1, device=device, dtype=torch.float32), 'step': 0,\n",
        "                                            'state_is_fp8': is_fp8, 'consecutive_soft_breaches': 0,\n",
        "                                            'state_scale': torch.ones(1, device=device, dtype=torch.float32)}\n",
        "            for _, p in block_params:\n",
        "                self.state[p]['exp_avg'] = torch.zeros_like(p, dtype=torch_state)\n",
        "                self.state[p]['amax_m'] = torch.zeros(1, device=device, dtype=torch.float32)\n",
        "        self._blocks_built = True\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        if not self._blocks_built: self._init_groups_and_states()\n",
        "        self.telemetry.clear()\n",
        "\n",
        "        for block_name, block_params, group in self.param_blocks:\n",
        "            block_state = self.block_states[block_name]\n",
        "            beta1, beta2 = group['betas']; lr, eps, wd = group['lr'], group['eps'], group['weight_decay']\n",
        "            block_state['step'] += 1; step = block_state['step']\n",
        "            first_param = block_params[0][1]\n",
        "            sum_g2 = torch.zeros(1, device=first_param.device, dtype=torch.float32); total_params_in_block = 0\n",
        "            for _, p in block_params:\n",
        "                if p.grad is None: continue\n",
        "                if not p.grad.is_leaf: warn_once(\"A gradient is not a leaf tensor. If using GradScaler, call scaler.unscale_(optimizer) before step().\")\n",
        "                total_params_in_block += p.numel()\n",
        "                grid = (triton.cdiv(p.numel(), 1024),); _reduce_g2_kernel_parallel[grid](p.grad, sum_g2, p.numel(), BLOCK_SIZE=1024)\n",
        "            if total_params_in_block == 0: continue\n",
        "\n",
        "            mean_g2 = sum_g2 / total_params_in_block; v_bar = block_state['v_bar_fp32']; v_bar.mul_(beta2).add_(mean_g2, alpha=1.0 - beta2)\n",
        "            bias_correction1 = 1.0 - beta1 ** step; bias_correction2 = 1.0 - beta2 ** step\n",
        "            v_bar_hat = v_bar / bias_correction2; denom = torch.sqrt(v_bar_hat) + eps\n",
        "            step_scale = (lr / denom).to(torch.bfloat16)\n",
        "\n",
        "            block_amax_m = 0.0; state_scale = block_state['state_scale']\n",
        "            for _, p in block_params:\n",
        "                if p.grad is None: continue\n",
        "                param_state = self.state[p]; m_buffer = param_state['exp_avg']; amax_m_buffer = param_state['amax_m'].zero_();\n",
        "                current_state_is_fp8 = block_state['state_is_fp8']\n",
        "                if current_state_is_fp8:\n",
        "                    current_state_dtype = 'fp8e4'\n",
        "                    if m_buffer.dtype != _DTYPE_MAP['fp8e4'][1]: m_buffer = param_state['exp_avg'] = m_buffer.to(_DTYPE_MAP['fp8e4'][1])\n",
        "                else: current_state_dtype = 'bf16';\n",
        "                if m_buffer.dtype != torch.bfloat16: m_buffer = param_state['exp_avg'] = m_buffer.to(torch.bfloat16)\n",
        "\n",
        "                _update_kernel[(triton.cdiv(p.numel(), 1024),)](\n",
        "                    p, p.grad, m_buffer, amax_m_buffer, lr, beta1, wd, 1.0 / bias_correction1, step_scale, state_scale.item(), p.numel(),\n",
        "                    T_MATH=_DTYPE_MAP[self.math_dtype_str][0], T_STATE=_DTYPE_MAP[current_state_dtype][0], STATE_IS_FP8=current_state_is_fp8)\n",
        "\n",
        "                current_amax = amax_m_buffer.item();\n",
        "                if current_amax > block_amax_m: block_amax_m = current_amax\n",
        "\n",
        "            if block_state['state_is_fp8']:\n",
        "                target_range = 120.0\n",
        "                new_scale = target_range / max(block_amax_m, 1e-6)\n",
        "                state_scale.mul_(0.9).add_(new_scale, alpha=0.1)\n",
        "\n",
        "            self.telemetry[block_name].append({'amax_m': block_amax_m, 'state_dtype': 'fp8' if block_state['state_is_fp8'] else 'bf16', 'scale': state_scale.item()})\n",
        "\n",
        "# ======================================================================\n",
        "# Main Execution Block\n",
        "# ======================================================================\n",
        "if __name__ == '__main__':\n",
        "    if not torch.cuda.is_available(): print(\"This benchmark requires a CUDA-enabled environment.\"); exit()\n",
        "    try:\n",
        "        from adam_mini import Adam_mini as AdamMiniOfficial\n",
        "    except ImportError:\n",
        "        print(\"Official Adam-mini not found. Please `pip install adam-mini` to run the full benchmark.\")\n",
        "        AdamMiniOfficial = None\n",
        "\n",
        "    device = \"cuda\"; dtype = torch.bfloat16\n",
        "\n",
        "    class SimpleTransformer(nn.Module):\n",
        "        def __init__(self, dim=1024, n_heads=16, mlp_dim=4096):\n",
        "            super().__init__(); self.dim = dim; self.n_heads = n_heads\n",
        "            self.embed = nn.Linear(512, dim); self.q_proj = nn.Linear(dim, dim); self.k_proj = nn.Linear(dim, dim); self.v_proj = nn.Linear(dim, dim)\n",
        "            self.o_proj = nn.Linear(dim, dim); self.mlp_1 = nn.Linear(dim, mlp_dim); self.mlp_2 = nn.Linear(mlp_dim, dim)\n",
        "            self.norm1 = nn.LayerNorm(dim); self.norm2 = nn.LayerNorm(dim)\n",
        "            self.lm_head = nn.Linear(dim, 512)\n",
        "        def forward(self, x):\n",
        "            x = self.embed(x)\n",
        "            x = x + self.o_proj(self.q_proj(self.norm1(x)) + self.k_proj(self.norm1(x)) + self.v_proj(self.norm1(x)))\n",
        "            x = x + self.mlp_2(torch.nn.functional.relu(self.mlp_1(self.norm2(x))))\n",
        "            return self.lm_head(x)\n",
        "\n",
        "    model_definition = lambda: SimpleTransformer().to(device).to(dtype)\n",
        "\n",
        "    def benchmark_optimizer(optimizer_class, name, steps=5000, **kwargs):\n",
        "        model = model_definition()\n",
        "\n",
        "        if name == \"Adam-mini (Official)\":\n",
        "            official_kwargs = {**kwargs, 'dim': model.dim, 'n_heads': model.n_heads}\n",
        "            optimizer = optimizer_class(model.named_parameters(), **official_kwargs)\n",
        "        elif name.startswith(\"Adam-mini (Triton\"):\n",
        "            optimizer = optimizer_class(model.parameters(), model_for_partitioning=model, **kwargs)\n",
        "        else:\n",
        "            optimizer = optimizer_class(model.parameters(), **kwargs)\n",
        "\n",
        "        print(f\"\\n--- Benchmarking {name} ---\")\n",
        "        for _ in range(10):\n",
        "            inp = torch.randn(64, 512, device=device, dtype=dtype)\n",
        "            optimizer.zero_grad(set_to_none=True); loss = model(inp).sum(); loss.backward(); optimizer.step()\n",
        "\n",
        "        torch.cuda.reset_peak_memory_stats(device); torch.cuda.synchronize(device)\n",
        "        start_time = time.time()\n",
        "        for i in range(steps):\n",
        "            inp = torch.randn(64, 512, device=device, dtype=dtype)\n",
        "            optimizer.zero_grad(set_to_none=True); loss = model(inp).sum(); loss.backward(); optimizer.step()\n",
        "        torch.cuda.synchronize(device); end_time = time.time()\n",
        "\n",
        "        peak_memory = torch.cuda.max_memory_allocated(device) / 1e6; total_time = end_time - start_time\n",
        "        print(f\"Total Time ({steps} steps): {total_time:.3f} s | Peak Memory: {peak_memory:.2f} MB\")\n",
        "\n",
        "    hyperparams = {'lr': 1e-3, 'eps': 1e-6, 'betas': (0.9, 0.999)}\n",
        "    print(\"\\n\\n\" + \"=\"*50); print(\" \" * 10 + \"Running Full Optimizer Benchmark\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    benchmark_optimizer(torch.optim.AdamW, \"AdamW (PyTorch)\", **hyperparams)\n",
        "\n",
        "    if AdamMiniOfficial:\n",
        "        benchmark_optimizer(AdamMiniOfficial, \"Adam-mini (Official)\", **hyperparams)\n",
        "\n",
        "    benchmark_optimizer(AdamMiniTriton, \"Adam-mini (Triton, BF16 State)\", state_dtype=\"bf16\", partition_fn=adam_mini_transformer_partition, **hyperparams)\n",
        "    benchmark_optimizer(AdamMiniTriton, \"Adam-mini (Triton, FP8 State)\", state_dtype=\"fp8e4\", partition_fn=adam_mini_transformer_partition, **hyperparams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCaD4Bju-rOP",
        "outputId": "4f97b010-f61e-4715-bd8b-42c5bdb90901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adam-mini\n",
            "  Downloading adam_mini-1.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Downloading adam_mini-1.1.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: adam-mini\n",
            "Successfully installed adam-mini-1.1.1\n",
            "\n",
            "\n",
            "==================================================\n",
            "          Running Full Optimizer Benchmark\n",
            "==================================================\n",
            "\n",
            "--- Benchmarking AdamW (PyTorch) ---\n",
            "Total Time (5000 steps): 31.564 s | Peak Memory: 153.57 MB\n",
            "Adam-mini found the param block with name: embed.weight torch.Size([1024, 512])\n",
            "Adam-mini found the param block with name: embed.bias torch.Size([1024])\n",
            "Adam-mini found the param block with name: q_proj.weight torch.Size([1024, 1024])\n",
            "Adam-mini found the param block with name: q_proj.bias torch.Size([1024])\n",
            "Adam-mini found the param block with name: k_proj.weight torch.Size([1024, 1024])\n",
            "Adam-mini found the param block with name: k_proj.bias torch.Size([1024])\n",
            "Adam-mini found the param block with name: v_proj.weight torch.Size([1024, 1024])\n",
            "Adam-mini found the param block with name: v_proj.bias torch.Size([1024])\n",
            "Adam-mini found the param block with name: o_proj.weight torch.Size([1024, 1024])\n",
            "Adam-mini found the param block with name: o_proj.bias torch.Size([1024])\n",
            "Adam-mini found the param block with name: mlp_1.weight torch.Size([4096, 1024])\n",
            "Adam-mini found the param block with name: mlp_1.bias torch.Size([4096])\n",
            "Adam-mini found the param block with name: mlp_2.weight torch.Size([1024, 4096])\n",
            "Adam-mini found the param block with name: mlp_2.bias torch.Size([1024])\n",
            "Adam-mini found the param block with name: norm1.weight torch.Size([1024])\n",
            "Adam-mini found the param block with name: norm1.bias torch.Size([1024])\n",
            "Adam-mini found the param block with name: norm2.weight torch.Size([1024])\n",
            "Adam-mini found the param block with name: norm2.bias torch.Size([1024])\n",
            "Adam-mini found the param block with name: lm_head.weight torch.Size([512, 1024])\n",
            "Adam-mini found the param block with name: lm_head.bias torch.Size([512])\n",
            "\n",
            "--- Benchmarking Adam-mini (Official) ---\n",
            "Adam-mini found 1 embedding layers, 1 output layers; 2 Querys and Keys;  1 Values;  1 attn_proj;  2 MLPs;\n",
            "Total Time (5000 steps): 30.385 s | Peak Memory: 115.83 MB\n",
            "\n",
            "--- Benchmarking Adam-mini (Triton, BF16 State) ---\n",
            "Hessian-aware partitioner created 9 blocks.\n",
            "Total Time (5000 steps): 46.129 s | Peak Memory: 99.13 MB\n",
            "\n",
            "--- Benchmarking Adam-mini (Triton, FP8 State) ---\n",
            "Hessian-aware partitioner created 9 blocks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4233490169.py:35: UserWarning: PyTorch version lacks fp8e4. Falling back to bf16.\n",
            "  warnings.warn(msg); _warn_once_cache.add(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Time (5000 steps): 46.086 s | Peak Memory: 99.13 MB\n"
          ]
        }
      ]
    }
  ]
}